1. Make a brief introduction about a variant of Transformer.
There is a variant of transfomer called Bidirectional Encoder Representations from Transformers(BERT), which is developed by Google in 2018 and applied to Google search engine in 2019. There are two variant models:
+ bert base: 12 encoders with 12 bidirectional self-attention heads
+ bert large: 24 encoders with 16 bidirectional self-attention heads.
2. Briefly explain why adding convolutional layers to Transformer can boost performance.
Transformer use the attention mechanism to combine information of all sequence and convolutional layers combine the local information. Thus, adding convolutional layers to Transformer can make the model to consider some local information and give the model some flexibility.